{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 宿題シート: CartPole (倒立振子)\n",
    "\n",
    "\n",
    "- このシートにはレポート課題に関する説明と，レポート課題の準備のための問題，そしてレポート課題の説明があります。\n",
    "- このシート自体を提出する必要はありません。レポート課題については別のシートを見てください。\n",
    "\n",
    "## CartPoleについて\n",
    "\n",
    "- CartPole環境の説明: [openAI gym::CartPole v0](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "- CartPoleのソース: [gym/gym/envs/classic_control/cartpole.py](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "- 以下の説明は[vmayoral:basic_reinforcement_learning/tutorial4/README.md](https://github.com/vmayoral/basic_reinforcement_learning/blob/master/tutorial4/README.md)を参考資料として作成したものです。\n",
    "\n",
    "## 準備: 基本モジュールの読み込み\n",
    "\n",
    "以下のモジュール読み込みを行っておくこと。これまでのシートでは，モジュールを読み込む必要があることを明示するために、同じモジュールを複数のセルで何度も読み込む指示を与えていたものもあるが、このシートでは初めのセルで一度読み込むだけにする。\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import math as ma\n",
    "\n",
    "# 以下はjupyter notebookでアニメーション表示をするための設定\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random action\n",
    "\n",
    "以下はCartPole環境に対して、ランダムに行動選択を行う場合のプログラム例である。\n",
    "\n",
    "FrozenLake環境では状態表示めに`env.render()`を使っていたが、CartPole環境での`env.render()`の出力は動画になり、そのままではjupyter notebook上には表示できない。そこで、jupyter notebook上に動画出力を行うための関数`render(env)`を以下のように用いる。\n",
    "\n",
    "```python\n",
    "def render(env):\n",
    "        plt.imshow(env.render(mode='rgb_array')) #取得した画像をimshow()で画面表示バッファに入れる\n",
    "        display.clear_output(wait=True) # 画面消去\n",
    "        display.display(plt.gcf()) # 画面表示バッファの内容を画面に出力\n",
    "        \n",
    "env = gym.make('CartPole-v0')\n",
    "for n_episode in range (2):\n",
    "    s = env.reset()\n",
    "    for t in range(100):\n",
    "        render(env)\n",
    "        a = env.action_space.sample() # ランダムに行動選択\n",
    "        s, r, terminal, info = env.step(a)\n",
    "        if terminal:\n",
    "            print(\"Episode {} finished after {} timesteps\".format(n_episode+1,t+1))\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境情報\n",
    "### state空間\n",
    "\n",
    "FrozenLake環境等と同様に，openAI gym標準の方法でstate, actionに関する情報を取得できる。\n",
    "```python\n",
    "import gym\n",
    "env=gym.make('CartPole-v0')\n",
    "\n",
    "# 状態の情報\n",
    "print(\"observation_space: \", env.observation_space)\n",
    "print(\"observation_space.high: \",env.observation_space.high)\n",
    "print(\"observation_space.low: \", env.observation_space.low)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### action空間\n",
    "\n",
    "```python\n",
    "print(\"action_space: \", env.action_space)\n",
    "print(\"action_space.n: \", env.action_space.n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他の情報\n",
    "\n",
    "- `gym.spec.max_episode_steps`: 1エピソード内の最大ステップ数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole環境の問題点\n",
    "- **問題点**: 状態空間が連続値をとる。\n",
    "- **対策**: 連続値のまま扱う方法もあるが，今回は，状態空間を離散値に分割する。いくつの状態空間に分割するべきかは考慮すべき点。\n",
    "\n",
    "**Q.** 状態空間をいくつかの離散値に分割することのメリットとデメリットを考えて説明しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 状態の各要素値を離散化する方法\n",
    "\n",
    "以下のNumpyの関数を使うと，連続値を簡単に離散化できる。\n",
    "\n",
    "- `numpy.linspace(start,stop,num)`\n",
    "    - `start`以上 `stop`以下の区間を`num`個の境界値で分割する(区間内のbin数は`num-1`，区間外も含めると`num+1`になる)\n",
    "    - 返り値は，分割した区間の境界の値(`start`, `stop`を含む`num`個)を要素とする`ndarray`配列\n",
    "- [`numpy.digitize(x,bins)`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.digitize.html)\n",
    "    - 引数:\n",
    "        - `x`: 離散化したい数値を要素とする一次元配列\n",
    "        - `bins`: 境界値を要素とする配列(`ndarray`)\n",
    "    - 返り値: 引数で渡した配列`x`の各要素`a`について，`bins[i-1] <= a < bins[i]`を満たす`i (i=1,...,len(bins)-1)`を配列にして返す。要素値が`[bins[0],bins[len(bins)-1]]`の範囲を超えていたら，その値に応じて0か最大値(`len(bins)`)となる。\n",
    "\n",
    "サンプルプログラム\n",
    "```python\n",
    "border = np.linspace(start=-2.0, stop=2.0, num=9)\n",
    "\n",
    "def to_bin(value, border): # valueを離散化し，そのインデックスを返す\n",
    "    index=np.digitize(x=[value], bins=border)\n",
    "    return index[0]\n",
    "```\n",
    "\n",
    "**Q.** 上記のサンプルプログラムを完成し，動作確認をしなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** 以下のようなクラス`Digitize`をつくなさい。前問の内容を参考にし，`numpy.linspace()`と，`numpy.digitize()`を使うこと。\n",
    "\n",
    "- 初期化メソッド:\n",
    "    - 引数: \n",
    "        - `start, stop, num`:\n",
    "            - `srart`以上`stop`以下の区間を`num`個の境界値で区切ることを指定するための引数\n",
    "        - 各引数のデフォルト値は自由に決めて良い\n",
    "    - 実行内容:\n",
    "        - 値`start`から`stop`までの閉区間を`num`個の境界値で均等に分割し，その境界値を要素とする`ndarray`配列を，インスタンス変数`border`に代入する。\n",
    "        - 注) `start`未満および，`stop`以上の区間も含めると，全部で`num+1`の区間に分割することになる\n",
    "- インスタンス・メソッド\n",
    "    - 名前: `bin`\n",
    "    - 引数:\n",
    "        - `value`: スカラー値\n",
    "    - 返り値\n",
    "        - 引数`value`が，インスタンス変数`border`によって決まる区間のどこに入るか，そのインデックスを返す。インデックスの値は，前問と同様。\n",
    "\n",
    "\n",
    "**動作確認:** 以下を確認しなさい。\n",
    "\n",
    "1. 以下を実行する\n",
    "\n",
    "```python\n",
    "class Digitize:\n",
    "    \"\"\"\n",
    "    この部分は自分で作る\n",
    "    \"\"\"\n",
    "state_digit=Digitize(start=-2.0, stop=2.0, num=9)\n",
    "```\n",
    "2. `state_digit.border==[-2., -1.5, -1., -0.5,  0.,0.5, 1.,1.5, 2. ])`であることを確認\n",
    "3. `len(state_digit.border)==9`であることを確認\n",
    "4. `state_digit.val(-2.1)==0`であることを確認\n",
    "5. 以下の各値がどうなるべきかを考え，実際にその値であることを確認\n",
    "```python\n",
    "state_digit.val(-2.1)\n",
    "state_digit.val(-1.9)\n",
    "state_digit.val(1.9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** cartpole環境の状態の各要素をそれぞれ10個の離散値にした場合，とりうる状態数は全部でいくつになるだろう?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q学習\n",
    "\n",
    "### QlearnerV2\n",
    "**課題**\n",
    "\n",
    "以下の設問に沿って CartPole環境用の学習プログラムを作成しなさい。\n",
    "\n",
    "**Q1.** 課題6-classにおいてFrozenLake環境用に作成したクラス`Qlearner`を次のセルにコピペし、以下のように修正しなさい。\n",
    "\n",
    "**注意:** クラス`Qlearner`は、第一回レポート用**ではなく**、**課題6-classの指示に従って作成したもの**(バグはとっておく)を使うこと。\n",
    "\n",
    "**修正点**\n",
    "\n",
    "- クラス名: `QlearnerV2`にする\n",
    "- 引数として受け取った状態変数をすでに作成したクラス`Digitize()`により，離散化する。\n",
    "    - **状態分割を行う区間は各自検討**すること。\n",
    "    - 離散化した4つの状態値を，Q関数の引数に与えるリストにするために，以下の関数`build_state()`を用いる\n",
    "- 関数`learning()`に、**引数`terminal=False`を追加**。この引数は現時点では処理には使わないが、各自必要に応じて利用して良い。\n",
    "\n",
    "以下に`class Qlearner`の**修正が必要な点のみ**を示す。\n",
    "\n",
    "```python\n",
    "index_list = lambda L, value: [i for i,x in enumerate(L) if x==value]\n",
    "\n",
    "def build_state(states): # 状態の各要素値のリスト[x,v,p,q]を文字列xvpqにする\n",
    "    return int(\"\".join(map(lambda state: str(int(state)), states)))\n",
    "\n",
    "class QlearnerV2: # 名前を変える!\n",
    "    def __init__(self, action_list, epsilon=0.1, alpha=0.2, gamma=0.9, q0=0.0):\n",
    "\n",
    "        # 中略\n",
    "        \n",
    "        N_BINS=10\n",
    "        self.dig_x=Digitize(start=???, stop=???, num=N_BINS-1)# ??? に入れる値は自分で決める\n",
    "        self.dig_v=Digitize(start=???, stop=???, num=N_BINS-1)\n",
    "        self.dig_ang=Digitize(start=???, stop=???, num=N_BINS-1)\n",
    "        self.dig_vang=Digitize(start=???, stop=???, num=N_BINS-1)\n",
    "\n",
    "    def reset(self): # 処理内容は各自記入\n",
    "        \n",
    "    def digitize(self,state): # 状態stateの各要素を離散化して、一つの文字列にする関数\n",
    "        x, v, ang, v_ang=state # タプルstateの各要素が状態値\n",
    "        return(build_state([self.dig_x.val(x), # 各状態値を離散化して文字列化\n",
    "                     self.dig_v.val(v),\n",
    "                     self.dig_ang.val(ang),\n",
    "                     self.dig_vang.val(v_ang)]))\n",
    "    def get_q(self, s, a): # 処理内容は各自記入\n",
    "\n",
    "    def get_maxQ(self,s): # 同上\n",
    "\n",
    "    def get_action(self, s):\n",
    "        s=self.digitize(s)\n",
    "        # 以下略\n",
    "        \n",
    "    def learning(self, s1, a1, r, s2, terminal=False): # 引数terminal追加\n",
    "        '''\n",
    "        Q-learning: Q(s, a) += alpha * (r + gamma*max(Q(s')) - Q(s,a))\n",
    "        '''\n",
    "        # get_action()を参考にs1,s2を離散化する命令文を追加する\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play_env_v2()\n",
    "**Q2.** シート`7-frozenlake`で作成した関数`play_env()`を修正して，状態値の離散化を行えるようにした関数`play_env_v2()`を作り，`Qlearner`によるCartPole環境の学習を行えるようにしなさい。\n",
    "\n",
    "- **変更点**は以下の通り\n",
    "    1. `env.render()`は以下のサンプルプログラムにある関数`render(env,i_episode,t)`の呼び出しに変更する。(理由は前述)\n",
    "    2. **引数の`max_step`は廃止**し、関数内の`max_step`は全てクラスオブジェクト`env`のインスタンス変数`env.spec.max_episode_steps`に置き換える。\n",
    "    3. 変数`history_steps`, `history_score`には**NumPyの`ndarray`を使う**\n",
    "    4. **変数`history_score`は、エピソード成功のときに1, 失敗のとき0**になるよう変更\n",
    "\n",
    "全体像は以下のようになる。\n",
    "```python\n",
    "def render(env,i_episode,t): # jupyter notebookに動画を表示するための関数\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    print(\"Episode={}, Time={}\".format(i_episode+1,t+1))    \n",
    "\n",
    "# 引数のデフォルト値は、まずは以下の値で試す\n",
    "def play_env_v2(env, agent, max_episode=2000, learning=True, render_step=100, verbose=False):\n",
    "    history_step=... # 要素数max_episodeのndarrayを定義。各要素の初期値は0にする\n",
    "    history_score=.. # 同上\n",
    "\n",
    "    # 以下は自分で書く(env.render()とhistory_scoreの設定変更忘れずに)\n",
    "\n",
    "# いよいよ実行!\n",
    "env=gym.make('CartPole-v0')\n",
    "env=wrappers.Monitor(env, './video',force=True) # ディレクトリvideoにアニメーションを保存するための設定\n",
    "\n",
    "Qagent=QlearnerV2(action_list=list(range(env.action_space.n)), \n",
    "                   epsilon=0.1, alpha=0.5, gamma=0.9, q0=0.0)\n",
    "history_step,history_score=play_env_v2(env,agent=Qagent,verbose=False)\n",
    "```\n",
    "\n",
    "**ビデオのファイル出力について**\n",
    "\n",
    "CartPoleを使うための宣言は`env=gym.make('CartPole-v0')`であるが，上記のように`wrappers.Monitor()`を追加するとmp4で適宜保存してくれるので，学習の途中経過をあとで動画で確認できる。\n",
    "保存タイミングは，env.step()を呼び出した回数が1000回以下のときには$0, 1^3, 2^3, 3^3,\\cdots$と$n^3$回ごと，それ以降は1000回ごとになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習結果の表示\n",
    "上述のプログラムが無事動いたかは以下で確認する。\n",
    "\n",
    "1. エラーはないか? ただし以下の警告文は無視して良い\n",
    "```\n",
    "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
    "```\n",
    "2. Matplotlibで, `play_env_cont()`の返り値の`history_step`や`history_score`を表示する。シート`6-class`で作成した`plot_history()`を使うのも良い。\n",
    "3. `./video`にできたビデオで確認する。\n",
    "\n",
    "**Q3.** 上記までで作ったプログラムでは、残念ながら**学習性能は低い**。何故かよく考え、まずは、報酬条件の変更を試みることで、学習性能を向上させなさい。\n",
    "\n",
    "- 報酬設定を変えるときには，エージェントのクラス`QlearnerV2`のインスタンス・メソッド`learning()`を修正し，引数で与えられる状態や報酬の情報をもとに再設定しなさい。\n",
    "- 詳しくはシート`7-frozenlake`の3.2参照。クラス`QlearnerV2`の派生クラスを作ると便利。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic法\n",
    "\n",
    "**Q.** 上記のクラス`QlearnerV2`を改造して、Actor-Critic法による学習エージェントのクラス`ActorCritic`を作りなさい。変更点は以下になる。\n",
    "\n",
    "- 初期化メソッドの変更\n",
    "    - 引数: `v0`を追加(状態価値関数Vの初期値)\n",
    "    - 処理内容\n",
    "        - インスタンス変数`v0`に引数`v0`の値を代入\n",
    "        - 価値関数$V(s)$の値を格納する空の辞書`V`を準備(Qlearnerの辞書`Q`の初期化と同様)\n",
    "- インスタンスメソッド`reset()`の修正\n",
    "    - 辞書`V`の要素を空にリセットする(辞書`Q`のリセットと同様)\n",
    "- インスタンスメソッド`get_V()`の追加\n",
    "    - 引数: `s` (状態)\n",
    "    - 返値: 辞書`V`を参照し、価値関数$V(s)$の値を返す。引数で与えられた状態`s`が辞書Vのキーにないときには、インスタンス変数`v0`の値を返す。(すでにある`get_Q()`とほぼ同様の内容)\n",
    "- インスタンスメソッド`learning()`の修正\n",
    "    - Q学習をActor-Critic法に変更する\n",
    "        $$\\delta=r+\\gamma V(s')-V(s)$$\n",
    "        $$V(s) \\leftarrow V(s)+\\alpha\\delta$$\n",
    "        $$p(s, a) \\leftarrow p(s,a) + \\alpha\\delta$$\n",
    "    - $p(s,a)$は、状態$(s,a)$における行動を決定するための関数\n",
    "        - 新たに辞書`p`を作ることはせず、`Qlearner`で使っていた辞書`Q`をそのまま使うことにする。行動決定も`get_action()`をそのまま利用可能\n",
    "    - $p(s,a)$の学習定数$\\alpha$は、$V(s)$の学習定数とは異なる値にすることもあるが、とりあえず同じ値にしておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Actor-Critic法による学習は、Q学習に比べて学習性能が良いかどうか検討しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax法\n",
    "**Q.** 以下のような関数`softmax_rand()`を作りなさい。\n",
    "\n",
    "- 引数:\n",
    "    - `V`: `ndarray`配列\n",
    "    - `T=1`: softmax法のパラメータ\n",
    "- 返値:\n",
    "    - 引数`V`の各要素値に対してsoftmax法を適用した確率で、そのインデックスをランダムに返す。\n",
    "    - 例) 引数が`l=[1,3,4]`の場合は，ランダムに各要素のインデックスを返すが、インデックス`k`が選択される確率$P(k)$ ($k=0,1,2$)は次式の通り。\n",
    "    \n",
    "    $$\\displaystyle P(k)=\\frac{e^\\frac{V(k)}{T}}{\\displaystyle\\sum_{i=0}^{N-1}e^\\frac{V(i)}{T}}$$\n",
    "\n",
    "- **動作確認:** 以下を確認しなさい。\n",
    "    - `softmax_rand(l=[1,1])`の返り値は0か1であり、その比率は1:1である。\n",
    "    - `softmax_rand(l=[0,log2,1])`の返り値は0か1か2であり、その比率は1:2:eである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** 前問で作った`softmax_rand()`の動作確認のため以下を行いなさい。\n",
    "\n",
    "1. 動作確認用のための以下のような関数`test()`を作りなさい。\n",
    "        - 引数\n",
    "            - `l`: リスト(要素は数値)\n",
    "            - `T=1`: softmax法のパラメータ\n",
    "            - `num`: 確率発生回数\n",
    "        - 動作\n",
    "            - 引数`l`,`T`を`softmax_rand()`に渡し、`num`回乱数を発生させる。その乱数値が、引数`l`の各要素のインデックスに何回該当したかを表示する。\n",
    "2. 関数`test()`を使って以下を確認しなさい。\n",
    "    - `softmax_rand(l=[1,1],T=1)`の返り値は0か1であり、その比率はほぼ1:1である。\n",
    "    - `softmax_rand(l=[0,log2,1],T=1)`の返り値は0か1か2であり、その比率はほぼ1:2:eである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** クラス`ActorCritic`の行動選択メソッド`get_action()`はε-greedy法になっているはずである。\n",
    "\n",
    "1. 行動選択をsoftmax法に変更したクラス`ActorCriticSofxmax`を、クラス`ActorCritic`の派生クラスとして作りなさい。もしくは、クラス`ActorCritic`の初期化メソッドで、ε-greedy法とsoftmax法を切り替えられるようにしなさい。\n",
    "2. softmax法の場合の学習効率はε-greedy法と比べて良いだろうか。softmax法のパラメータ$T$はどのような値が良いのだろう。特に良い値があるとしたら、その理由はどうしてだろう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[課題]\n",
    "- 報酬の設定方法次第で学習効率はいろいろ変わる。例えば，タスク失敗時の報酬を-5とすると，どのように学習速度は変わるだろうか。\n",
    "- 離散化による状態分割数は多くするのと少なくするのとどちらが良いだろうか\n",
    "- 学習の結果，どのような行動価値関数が獲得されただろうか\n",
    "- 行動の決定に，greedy法，ε-greedy法，softmax法のどれを用いるのがいいのだろうか\n",
    "- Actor-Critic法を使うと，学習速度は改善するだろうか\n",
    "- 学習速度を上げるには他にどのような工夫をできるだろうか。報酬設定だけではなく，いろいろな可能性を考えて検証しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
