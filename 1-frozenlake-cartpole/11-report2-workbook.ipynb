{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二回レポート課題\n",
    "\n",
    "\n",
    "第二回の課題は，`CartPole`をよりよい成績で学習するActor-Critic法の学習エージェントのクラスを作成することです。提出物等は，以下を通りです。\n",
    "\n",
    "1. **プログラムの提出** : このシートを利用します。\n",
    "2. **レポートの提出**: 詳細はelearningのページを見てください。\n",
    "3. **レポートの相互採点**: 後日演習時に説明します。\n",
    "\n",
    "**[重要]** 第二回レポートは**単位取得の必須条件**です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提出レポートについて\n",
    "\n",
    "\n",
    "### フォーマットについて\n",
    "\n",
    "1. **A3, 1頁(片面)**\n",
    "2. **pdf形式で提出**\n",
    "3. レポートには**学籍番号や氏名はレポート中に記入しないこと**\n",
    "\n",
    "### 書いてはいけないこと\n",
    "\n",
    "1. **`CartPole`の環境の概要**(環境やルールの説明)**は書かないこと**\n",
    "2. **`python`のプログラムは書いてはいけない**。**関数名や変数名も書いてはいけない。**\n",
    "\n",
    "\n",
    "### 書くべきこと\n",
    "\n",
    "1. ActorCritic法について(簡潔に)\n",
    "2. 学習エージェントの学習性能の向上のために工夫した点\n",
    "    1. **提案方法:** どんな方法を考えたか。(複数の異なる方法を検討し，検討すること。)\n",
    "    2. **検討手法:** 考えたいくつかの方法のいずれが良いかを判定したデータ\n",
    "    3. **考察:** 良い結果を出した方法が，何故他の方法に良いのか，その理由に関する考察。\n",
    "\n",
    "\n",
    "### 気をつけるべきこと\n",
    "\n",
    "**商品の宣伝のチラシのように見やすいデザイン**にすること。\n",
    "\n",
    "- たくさん書いてあってもわかりにくいのはダメ\n",
    "- 情報が少なすぎるのもダメ。\n",
    "\n",
    "### レポートの採点について\n",
    "\n",
    "- **レボートのデザイン， 内容について以下の点に留意して相互採点**していただきます。\n",
    "- 他者のレポートを採点した内容と、他者からの評価内容の両方が成績に反映します。\n",
    "- 以下で，**単に文章を枠で囲んだだけのものは図とはみなしません**\n",
    "\n",
    "\n",
    "1. **レポートのデザイン**\n",
    "    1. **明瞭性:** 図や文章が用紙内に見やすくなるよう，**レイアウトの工夫**がなされているか\n",
    "\n",
    "2. **図**\n",
    "    1. **明瞭性:** 個々の図は色分けやデザインを工夫して、その**内容が分かりやすい**ものになっているか。また，各図について、**簡潔かつ分かりやすい解説**がなされているか。    \n",
    "    1. **正確性:** グラフであれば，**各軸のタイトルや目盛り**は適切に表示されているか。\n",
    "\n",
    "3. **ActorCritic法の説明**\n",
    "    1. **明瞭性:** ActorCritic法について知らない人にも**わかりやすい説明**になっているか。\n",
    "    2. **正確性:** ActorCritic法についての説明に**間違いはない**か。\n",
    "\n",
    "4. **提案方法**\n",
    "    1. **明瞭性:** 学習性能を上げるために検討した**方法を具体的にわかりやすく説明**しているか。\n",
    "    2. **多様性:** (一つだけではなく)**さまざまな，有効に思われる手法**を考えているか。\n",
    "    3. **独自性:** 提案手法には，**独自の工夫**がなされているか。独創的なものではなくても，**独自の調査**に基づくものか。\n",
    "\n",
    "5. **検討手法**\n",
    "    1. **妥当性:** 考えたいずれの方法がより良いものかを判断するための検討**手法は適切**か。また，検討**結果(主張)は適切**か。\n",
    "    1. **証拠の明示:** 検討結果を示す図等が明示されているか。\n",
    "\n",
    "6. **考察**\n",
    "    1. **根拠の明瞭性:** 検討の結果，良い学習方法と判断された方法が，他の方法に比べて**なぜ良いのか，その理由**が十分に考察され，わかりやすく説明されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole プログラム提出について\n",
    "\n",
    "`CartPole-v0`に対して作成した，最もパフォーマンスの良い学習エージェントプログラムを，以下の指示に従って提出してください。\n",
    "\n",
    "**課題1.** このファイルの名前を以下のように変更しなさい。\n",
    "\n",
    "```\n",
    "CartPole-<学籍番号>-<名前>.ipynb\n",
    "例) CartPole-16-2202-099-9-YamaguchiDai.ipynb\n",
    "```\n",
    "\n",
    "- 名前は必ず，姓名の順にすること\n",
    "- **ファイル名が不適切な場合は減点**します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 名前の変数定義\n",
    "\n",
    "**課題2** 次の実行セルに，以下のように`id`と`name`を定義してください。\n",
    "\n",
    "- 学籍番号や名前の記載方法は，上記のファイル名と同様の注意を守ること\n",
    "\n",
    "```\n",
    "number=\"学籍番号\"\n",
    "name=\"名前\"\n",
    "```\n",
    "以下は名前が\"山口 大\"の場合の例\n",
    "```\n",
    "number=\"16-2202-099-9\"\n",
    "name=\"YamaguchiDai\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "**課題3**\n",
    "\n",
    "次のセルでは，モジュールや`CartPole`の読み込みと，評価に使うためのプログラムの定義をしています。\n",
    "\n",
    "- **randomモジュール**はここで読み込んでいます。学習エージェントのクラス定義に必要な場合は，モジュール名`ra`でアクセスしてください。\n",
    "- **この課題の内容**: 次のセルは**Rawセル**になっています。これを**Codeセルに変更して実行**してください。ただし，**セルの内容を編集してはいけません。編集した場合は不正行為**とみなします。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import math as ma\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "actions=list(range(env.action_space.n))\n",
    "\n",
    "def plot_history(step,score):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.rcParams[\"font.size\"] = 16\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"mean steps\")\n",
    "    plt.ylim(0,env.spec.max_episode_steps)\n",
    "    plt.plot(step)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"success rate\")\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylim(0,max(score))\n",
    "    plt.plot(score)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run_episodes(env, agent, max_episode=200, learning=True):\n",
    "    history_step=np.zeros(max_episode)\n",
    "    history_score=np.zeros(max_episode)\n",
    "    \n",
    "    for n_episode in range(max_episode):\n",
    "        ra.seed()\n",
    "        s=env.reset()\n",
    "        for t in range(env.spec.max_episode_steps):\n",
    "            s_old=s\n",
    "            action=agent.get_action(s)\n",
    "            s,r,terminal,_=env.step(action)\n",
    "            if learning==True: agent.learning(s_old,action,r,s,terminal)\n",
    "            if terminal: break\n",
    "        history_step[n_episode]=t+1\n",
    "        if t==env.spec.max_episode_steps-1 : history_score[n_episode]=1\n",
    "\n",
    "    return(history_step,history_score)\n",
    "\n",
    "def analyze_scores(steps,scores,condition):\n",
    "    goal_steps=140\n",
    "    goal_score=0.3\n",
    "    \n",
    "    above_steps=np.where(steps>goal_steps)\n",
    "    above_duration_steps=len(above_steps[0])\n",
    "    min_above_step=-1 if above_duration_steps==0 else np.nanmin(above_steps)\n",
    "\n",
    "    above_scores=np.where(scores>goal_score)\n",
    "    above_duration_scores=len(above_scores[0])\n",
    "    min_above_score=-1 if above_duration_scores==0 else np.nanmin(above_scores)\n",
    "    \n",
    "    print(\"[Episode steps]\")\n",
    "    print(\"over %d mean steps at index: %d\" % (goal_steps,min_above_step))\n",
    "    print(\"# of over %d mean steps: %d\" % (goal_steps,above_duration_steps))\n",
    "    print(\"\\n[Score: Success rate]\")\n",
    "    print(\"over %3.2f mean score at index: %d\" % (goal_score,min_above_score))\n",
    "    print(\"# of over %3.2f mean score: %d\" % (goal_score,above_duration_scores))\n",
    "    print(\"maximum mean score: %4.3f\" % (max(scores)))\n",
    "    print(\"final mean score: %4.3f\" % (scores[len(scores)-1]))\n",
    "\n",
    "    fname=\"CartPole-\"+ID+\"-\"+name+\".dat\"\n",
    "    with open(fname, mode='w') as f:\n",
    "        f.write(\"{},{},{},{},{},{},{},{}\".format(\n",
    "            id,\n",
    "            name,\n",
    "            above_duration_steps,\n",
    "            min_above_step,\n",
    "            above_duration_scores,\n",
    "            min_above_score,\n",
    "            max(scores),\n",
    "            scores[len(scores)-1]\n",
    "        )+condition)\n",
    "    \n",
    "def mean_performance(env, agent, n_trial=30, max_episode=1000):\n",
    "    window_size=env.spec.trials\n",
    "    filter = np.ones(window_size)/window_size\n",
    "    max_episode0=max_episode\n",
    "    max_episode+=window_size-1\n",
    "    \n",
    "    mean_steps=np.zeros(max_episode)\n",
    "    mean_scores=np.zeros(max_episode)\n",
    "\n",
    "    for trial in range(n_trial):\n",
    "        agent.reset()\n",
    "        steps,scores=run_episodes(env=env, agent=agent, max_episode=max_episode)\n",
    "        mean_steps+=steps\n",
    "        mean_scores+=scores\n",
    "        print(\"+\",end=\"\") if (trial%5==4) else print(\"-\",end=\"\")\n",
    "    print()\n",
    "    \n",
    "    mean_steps=np.convolve(mean_steps, filter, mode='valid')\n",
    "    mean_scores=np.convolve(mean_scores, filter, mode='valid')\n",
    "    mean_steps/=(n_trial+1)\n",
    "    mean_scores/=(n_trial+1)\n",
    "    \n",
    "    print(\"# of trials: %d, \" % (n_trial), end='')\n",
    "    print(\"episodes/trial: %d, \" % (max_episode0), end='')\n",
    "    print(\"steps/episode: %d, \" % (env.spec.max_episode_steps), end='')\n",
    "    print(\"window size (steps): %d\" % (window_size))\n",
    "    condition=\"{}-{}-{}-{}\".format(n_trial,max_episode0,env.spec.max_episode_steps,window_size)\n",
    "    analyze_scores(mean_steps,mean_scores,condition)\n",
    "\n",
    "    return(mean_steps,mean_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プログラム提出セル\n",
    "\n",
    "**課題4** 次のセルに学習エージェントのクラス定義を記入してください。**クラス名は`ActorCritic`としてください。**\n",
    "\n",
    "- 必要に応じて，`ActorCritic`が呼び出す関数の定義も記入してください。おそらく、シート`10-cartpole`で作成したクラス`Digitize`も必要です。\n",
    "- **注意1:** これまでの課題の指示に従って作った`ActorCritic`の各関数の名前や引数名は変更しないでください。**性能評価ができない**場合があります。その場合の**評点は0点**になります。気をつけてください。\n",
    "- **注意2:** `ActorCritic`にバグがあって正常動作しないときにも**評点は0点**になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価用セル\n",
    "\n",
    "**課題5.** 以下は提出プログラムの評価のための命令文です。\n",
    "\n",
    "- この命令文を次のセルにコピペして，**実行できることを確認**してください。\n",
    "- **実行時にエラーがでたら採点対象外**になります。\n",
    "- **命令文を変更してはいけません**。\n",
    "\n",
    "**評価プログラムについて**\n",
    "- 以下で呼び出している`mean_performance()`は，学習クラス`ActorCritic`の評価を以下のように行う。\n",
    "    - 1回の学習シミュレーション: 「初期状態から1000エピソードの学習」\n",
    "    - 上記学習シミュレーションを30回繰り返し，報酬推移(エピソード終了)の平均を計算\n",
    "    - 100エピソード中の**エピソード成功率の平均が0.3を超えたらプログラムは合格点**(6/10)。成績に応じて追加点\n",
    "    - 上記成績を得られなかったら，プログラムは0点\n",
    "    - 実行結果は，ファイル`CartPole-<学籍番号>-<名前>.dat`にも書き出される\n",
    "- `mean_performance()`の実行にはすこし時間がかかるが，引数設定により上記のエピソード回数や平均回数を小さくすれば短縮できる。ただし，`mean_performance()`の**引数を変えて試す場合は，提出用シートでは実行しないこと**。\n",
    "\n",
    "```python\n",
    "%%time\n",
    "ACagent=ActorCritic(action_list=actions)\n",
    "steps,scores=mean_performance(env,agent=ACagent)\n",
    "plot_history(steps,scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学生用シートはここから下は削除"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 再度確認するとき\n",
    "%%time\n",
    "ACagent.reset()\n",
    "steps,scores=mean_performance(env,agent=ACagent)\n",
    "plot_history(steps,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic 報酬変更条件(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActorCrtic (softmax, 報酬デフォルト)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ActorCritic (softmax, 報酬変更版)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q学習のテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q学習報酬条件変更版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gym.openai.com/evaluations/eval_JzIX5Ws5QTef1GHqFLY8Q/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
