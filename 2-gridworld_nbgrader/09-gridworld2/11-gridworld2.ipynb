{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "# 名前欄\n",
    "\n",
    "</div>\n",
    "\n",
    "以下に名前と学籍番号(STUDENT_ID)の入力をして下さい。\n",
    "\n",
    "例)\n",
    "```python\n",
    "FAMILY_NAME = \"Yamaguchi\"   # 姓をローマ字で入力\n",
    "FIRST_NAME = \"Daisuki\"   #  名をローマ字で入力\n",
    "USER = \"a099bc\"          # 大学アカウントのユーザ名\n",
    "STUDENT_ID = \"20-2202-000-0\" # 省略なしで入力\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAMILY_NAME = \"\"   # 姓をローマ字で入力\n",
    "FIRST_NAME = \"\"   #  名をローマ字で入力\n",
    "USER = \"\"          # 大学アカウントのユーザ名\n",
    "STUDENT_ID = \"\"  # 12-2202-000-0 等と省略なしで入力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "# 注意\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "- 以下の説明文を読み， 内容を納得できるように自分で試しなさい。\n",
    "- **「# このセルで試してみよう」**, **「# このセルで動作確認をしよう」**と書かれているセルは自由に使って構いません。\n",
    "- 必要に応じてセルを増やしても構いません。役立ちそうな情報もメモしておくと，今後プログラムを組む上で便利です。\n",
    "- 表題等には適宜教科書のページを書いてあります。詳しい説明は教科書を参照しなさい。\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "## 設問と解答用セル\n",
    "    \n",
    "</div>   \n",
    "\n",
    "\"**Q.**\" から始まる設問に対する解答は必ず入力しなさい。解答用セルには以下のように表示される。\n",
    "\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "\n",
    "**注意**\n",
    "\n",
    "- <span class=\"mark\">raise NotImplementedError()の一行は**必ず消去**して</span>，<span class=\"mark\"># YOUR CODE HERE の下に解答</span>を書くこと。\n",
    "- <span class=\"mark\">\\# YOUR CODE HERE の一行は**消さない**</span>こと。どこに解答を書くべきかがわからなくなることがあります。\n",
    "- なんらかの演算をしたり，命令文を作ったときは，その**結果が予想通りか否かを必ず確認しなさい**\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "## 評価用セル\n",
    "    \n",
    "</div>\n",
    "\n",
    "- **「評価用セル」と書かれているセルは採点のためのセル**です。\n",
    "- **評価用コードが一部表示されている場合，それを実行すると課題の実行結果の確認**をできます。\n",
    "- **表示されていない評価用コードも**あります。\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "## 課題提出前の注意\n",
    "    \n",
    "</div>   \n",
    "\n",
    "**[重要]** 以下の手順で，全ての課題が正常に動作するか確認してから提出しましょう。\n",
    "\n",
    "1.  メニューバーの\"Kernel\"をクリック後，\"Restart\"を選択\n",
    "2. メニューバーの\"Cell\"を，\"Run All\"を選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "468b8b9a82f13ced7fcb16debd13d3aa",
     "grade": false,
     "grade_id": "cell-3cf93821ee8706d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# このセルは自動評価を行うための命令です。内容は無視して，実行(Shift-Enter)のみをしてください。\n",
    "#%%writefile unittests.py\n",
    "#%load unittests.py\n",
    "%run unittests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# 第二回レポート課題の準備\n",
    "\n",
    "</div>\n",
    "\n",
    "このシートでは以下を整備します。\n",
    "\n",
    "1. Q学習エージェント: `class Qlearner` (作成済みのものをコピペで持ってくる)\n",
    "2. 強化学習の実行用関数: `play_env()`(作成済みのものをコピペで持ってきて，少し修正)\n",
    "3. 結果の可視化: `plot_history()`(作成済みのものをコピペで持ってくる)\n",
    "4. 結果の分析: `mean_performance()`(`play_env()`を何度も実行して，結果の平均と標準偏差を求める)\n",
    "5. 結果の可視化その2: `plot_history()`(コピペで持ってきたのを少し変えて，標準偏差も表示できるようにする)\n",
    "6. SARSA学習エージェント `class SARSAlearner` (すでに作ったものをコピペで持ってくる)\n",
    "7. Actor-Critic学習エージェント(`class Qlearner`を修正して作る)\n",
    "8. `MDPGridworld-v3`で遊ぶ\n",
    "\n",
    "まずは，これまでと同様に`MDPGridworld-v1`環境を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルでは，実行に必要なモジュールの読み込みを行う。\n",
    "# ここに書かれているもの以外に必要なものがあれば，適宜追加すること。\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# Q学習エージェント\n",
    "\n",
    "</div>\n",
    "\n",
    "**Q.** 以前の課題シート(gridworld)で作ったクラス`Qlearner`を入力しなさい。独自仕様のインスタンス関数等が追加されていても良いが，行動選択にはε-greedy法を使っているものを入力すること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29e0a788f91eeea6af50e3421d9a6e3d",
     "grade": false,
     "grade_id": "cell-1485191ebed626ef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 以下にclass QLearnerの定義を書きなさい。(必要ならindex_list()等, Qlearnerの実行に必要な関数も書きなさい)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# 強化学習の実行用関数\n",
    "\n",
    "</div>\n",
    "\n",
    "## エージェントを動かす関数: `play_env()`\n",
    "**Q.** 以前の課題シート(gridworld)で作った関数`play_env()`を，以下のように修正して，次のセルに入力しなさい。\n",
    "\n",
    "**変更点**\n",
    "- 変数`history_steps`, `history_score`にはリストではなく，**NumPyの`ndarray`を使う**。(全要素を0に初期化するための命令を`np.zeros()`を使って書き直す)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86708b2b5842a0eacc852808c6520112",
     "grade": false,
     "grade_id": "cell-c910cfdb8bbb87ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def play_env(env, agent, learning=True, max_episode=30, render_step=20, verbose=False):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルで試してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687622ba9d04f639c9025e4fed7fa21e",
     "grade": true,
     "grade_id": "E-check-play_env",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル(返り値がndarrayか確認)\n",
    "env = gym.make(\"MDPGridworld-v1\")\n",
    "action_list = list(range(env.action_space.n))  # action_list取得\n",
    "agent = Qlearner(action_list)  # 学習エージェント生成\n",
    "\n",
    "step, score = play_env(env=env, agent=agent, max_episode=100, verbose=False)\n",
    "eq_(type(step), type(np.ones(1)))\n",
    "eq_(type(score), type(np.ones(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の可視化: `plot_history()`\n",
    "\n",
    "Q. 次のセルにgridworld課題シートで作った関数`plot_history()`を入力して動作確認を行い，必要に応じて修正をしなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14f81b58d9037e3b0fb2ff75b3c64a59",
     "grade": false,
     "grade_id": "cell-4937f30c0d392b0b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 次のセルを実行して，ここまでに入力した内容の動作確認をしなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルで動作確認をしよう\n",
    "# Qlearner + play_env() + MGPGridWorld の学習プログラム\n",
    "\n",
    "env = gym.make(\"MDPGridworld-v1\")\n",
    "action_list = list(range(env.action_space.n))  # action_list取得\n",
    "agent = Qlearner(action_list)  # 学習エージェント生成\n",
    "\n",
    "step, score = play_env(env=env, agent=agent, max_episode=100, verbose=False)\n",
    "plot_history(step, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ecc155fa99ba58fff72e2e266f713fe",
     "grade": true,
     "grade_id": "cell-4c6632fdd17d4004",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の分析: `mean_performance()`\n",
    "\n",
    "Q. 上記のプログラムを何度か実行するとわかるように，強化学習には乱数を用いるので，実行するたびに若干結果が異なる。そこで，学習性能の評価のためには，関数`play_env()`で実行した結果を何度も実行し，返り値の`step`, `score`の平均値と標準偏差を評価する必要がある。このような評価を行うための関数`mean_performance()`を以下のように作成しなさい。\n",
    "\n",
    "\n",
    "**引数とデフォルト値**\n",
    "\n",
    "- `env`: 学習環境。`gym.make(\"MDPGridworld-v1\")`の返り値などを渡す\n",
    "- `agent`: 環境を動く行動を決定するエージェント。`Qlearner`などのインスタンス変数を渡す\n",
    "- `n_trial`: `play_env()`を実行する回数(デフォルト値`10`)\n",
    "- `max_episode`: 繰り返すエピソード数 (デフォルト値`100`)\n",
    "\n",
    "**処理**\n",
    "\n",
    "- `play_env()`を引数で指定した回数(`n_trial`)だけ実行する。`play_env()`の引数には，`mean_performance()`の引数(`env`, `agent`, `max_episode`)をそのまま渡す他，`learning=True`, `verbose=False`を指定する。\n",
    "- **新たに`play_env()`を呼び出す毎に`agent.reset()`によって学習内容を消去**すること。\n",
    "- 上記の実行の結果，`play_env()`の返り値`history_step`, `history_score`が`n_trial`回得られる。各データにおける第1,2,...,`max_episode`回目のエピソードのそれぞれについて平均値と標準偏差を求める。\n",
    "- 返り値平均値と標準偏差は長さ`max_episode`の`ndarray`になることに注意\n",
    "- **ヒント**) `numpy`課題シートにある`sinrnd()`の平均値を求める問を参考にすると良い\n",
    "\n",
    "        \n",
    "**返り値**\n",
    "\n",
    "- `step_mean`: 各エピソードにおいてエピソード終了までにかかったステップ数の平均値\n",
    "- `score_mean`: 各エピソードにおいて獲得した総報酬の平均値\n",
    "- `step_sd`:  各エピソードにおいてエピソード終了までにかかったステップ数の標準偏差\n",
    "- `score_sd`: 各エピソードにおいて獲得した総報酬の標準偏差\n",
    "\n",
    "**実行例**\n",
    "```python\n",
    "agent = Qlearner(action_list)\n",
    "step_mean, score_mean, step_sd, score_sd = mean_performance(\n",
    "    env=env, agent=agent, n_trial=10, max_episode=100\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1f73478c71c0d97a302d9014019d0bd",
     "grade": false,
     "grade_id": "cell-e6ced5234d014b2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルで動作確認をしよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4563924a2c82404719bc7dad1ae3b1b",
     "grade": true,
     "grade_id": "E-mean_performance",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セルその2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の可視化その2: `plot_history()`の修正\n",
    "\n",
    "Q. 関数`plot_history()`を以下のように修正して、`mean_performance()`で得られた`steps`と`scores`の`mean±SD`のグラフを表示できるようにしなさい。\n",
    "\n",
    "**引数**\n",
    "\n",
    "- `mean_performance()`の4つの返り値を引数として渡せるように、引数には`step`, `score`に以下を追加。\n",
    "    - `step_sd` : デフォルト値は`None` \n",
    "    - `score_sd`: デフォルト値は`None`\n",
    "    - 注意: \n",
    "        - ある変数値が`None`である場合，その**変数が未定義**であることを表す。なお，`None`は**文字列ではない**ことに注意。\n",
    "        - `if`文で「変数`a`が未定義でない」場合の条件分岐を書くには以下のように書く。\n",
    "        ```python\n",
    "        if a is not None:\n",
    "            ...\n",
    "        ```\n",
    "\n",
    "**処理**\n",
    "\n",
    "- 元の`plot_history()`と同様に引数`step`と`score`に対するグラフをそれぞれ座標コンテナ`ax1`と`ax2`に登録して表示する他、以下の例のように、`step±step_sd`と`score±score_sd`のグラフもそれぞれの上下に表示する。\n",
    "- ただし，`step_sd`や`score_sd`の値が`None`の場合には，該当する標準偏差のグラフは表示しない。\n",
    "\n",
    "**返り値**\n",
    "\n",
    "- 元の`plot_history()`と同様に座標コンテナ`ax1`と`ax2`を返り値とする。\n",
    "\n",
    "\n",
    "**実行例**\n",
    "```python\n",
    "agent = Qlearner(action_list)\n",
    "step_mean, score_mean, step_sd, score_sd = mean_performance(env=env, agent=agent, n_trial=10, max_episode=100)\n",
    "plot_history(step_mean, score_mean, step_sd, score_sd)\n",
    "```\n",
    "\n",
    "![](images/plot_history2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c71040ad11b42fe6450bc148b50d51de",
     "grade": false,
     "grade_id": "cell-e0f8d259f7107d2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルで試してみよう\n",
    "agent = Qlearner(action_list)\n",
    "step_mean, score_mean, step_sd, score_sd = mean_performance(env=env, agent=agent, n_trial=10, max_episode=100)\n",
    "plot_history(step_mean, score_mean, step_sd, score_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12ebf95aab32a8f785aff750ae80d84f",
     "grade": true,
     "grade_id": "E-plot_history2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# SARSA学習エージェント\n",
    "\n",
    "</div>\n",
    "\n",
    "Q. 課題シート'gridworld'で，`class Qlearner`の継承クラスとして作成した`class SARSAlearner`を次のセルにコピーしなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32f8a1fc3ee4603ee9f1a03525a98dda",
     "grade": false,
     "grade_id": "cell-978e927d584acc4f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルで動作確認をしよう\n",
    "agent = SARSAlearner(action_list)\n",
    "step_mean, score_mean, step_sd, score_sd = mean_performance(env=env, agent=agent, n_trial=10, max_episode=100)\n",
    "plot_history(step_mean, score_mean, step_sd, score_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ebfc831634a7811a5ddf70fe00e3f9d",
     "grade": true,
     "grade_id": "E-SARSA",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# Actor-Criticエージェント\n",
    "\n",
    "</div>\n",
    "\n",
    "**Q.** `class Qlearner`を改造して、Actor-Critic法による学習を行う`class ActorCritic`を作りなさい。**変更点は以下**になる。\n",
    "`class Qlearner`の継承クラスとして作成しても良いが，新規に作り直しても良い。\n",
    "\n",
    "\n",
    "- 初期化メソッドの修正・追加\n",
    "    - 引数:\n",
    "        - `v0`を追加。状態価値関数$V(s)$の初期値, デフォルト値は0.0\n",
    "        - `T`を追加。行動選択に用いるsoftmax法のパラメータ, デフォルト値は1.0\n",
    "        - 行動選択にはsoftmax法を利用するので，ε-greedy法のパラメータに用いていた引数`epsilon`は不要になる。ActorCriticの初期化メソッドの引数からは削除。\n",
    "    - 処理内容\n",
    "        - インスタンス変数`v0`に引数`v0`の値を代入\n",
    "        - 状態価値関数$V(s)$の値を格納する空の辞書`V`を準備。(Qlearnerの辞書`Q`の初期化と同様)\n",
    "        - その他の処理は`Qlearner`と同じ\n",
    "- インスタンスメソッド`reset()`の追加処理\n",
    "    - 辞書`V`の要素を空にリセットする(辞書`Q`のリセットと同様)\n",
    "- インスタンスメソッド`get_V()`の追加\n",
    "    - 引数: `s` (状態)\n",
    "    - 返値: 辞書`V`を参照し、価値関数$V(s)$の値を返す。引数で与えられた状態`s`が辞書`V`のキーにないときには、インスタンス変数`v0`の値を返す。(すでにある`get_Q()`とほぼ同様の内容)\n",
    "- インスタンスメソッド`learning()`の修正\n",
    "    - Q学習をActor-Critic法に変更する。\n",
    "        $$\\delta=r+\\gamma V(s')-V(s)$$\n",
    "        $$V(s) \\leftarrow V(s)+\\alpha\\delta$$\n",
    "        $$Q(s, a) \\leftarrow Q(s,a) + \\alpha\\delta$$\n",
    "        ただし，`s'`がエピソード終了状態ならば\n",
    "        $$\\delta=r-V(s)$$\n",
    "    - $Q(s,a)$は、状態$(s,a)$における行動を決定するための関数\n",
    "        - 講義では関数`p(s,a)`と説明していた関数。Q学習の状態行動価値関数とは定義が異なるが，`Qlearner`で使っていた辞書`Q`をそのまま用いることにする。\n",
    "    - $Q(s,a)$の学習定数$\\alpha$は、$V(s)$の学習定数とは異なる値にすることもあるが、とりあえず同じ値にしておく。\n",
    "- 行動選択メソッド`get_action()`の修正\n",
    "    - $Q(s,a)$に基づく行動選択をsoftmax法にする。すなわち，行動`a`の選択確率は次式で与えられる。\n",
    "        $$\\displaystyle P(a)=\\frac{e^\\frac{Q(a)}{T}}{\\displaystyle\\sum_{i=1}^{N}e^\\frac{Q(i)}{T}}$$\n",
    "    - softmax法による行動決定に使う関数`softmax_rand()`には，課題シート`numpy`ですでに作成したものを用いなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "539f6dd72641a673f2de8cdb36fe0aac",
     "grade": false,
     "grade_id": "cell-d728b712c482564c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# このセルに課題シート`numpy`で作成した関数softmax_rand()を入力する。\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a7d09756e0978518be06704310dfec0",
     "grade": false,
     "grade_id": "cell-9777873e0e4cd7ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# このセルにclass ActorCriticの定義を入力する\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23ebeda2a7f7f3b72e0fdcb4414a8e9e",
     "grade": false,
     "grade_id": "cell-5953d114bf063069",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# このセルで，`play_env()`と`plot_history()`を用いて動作確認をしよう\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93fee2b904a8e37dd604c1d16369c2b7",
     "grade": true,
     "grade_id": "E-ActorCritic1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル1\n",
    "action_list = [\"left\", \"right\"]\n",
    "agent = ActorCritic(action_list,\n",
    "        alpha=0.2,\n",
    "        gamma=0.9,\n",
    "        v0=0.0,\n",
    "        q0=0.0,\n",
    "        T=1.0)\n",
    "\n",
    "# check init()\n",
    "eq_(agent.alpha, 0.2)\n",
    "eq_(agent.gamma, 0.9)\n",
    "eq_(agent.q0, 0.0)\n",
    "eq_(agent.v0, 0.0)\n",
    "eq_(agent.T, 1.0)\n",
    "\n",
    "eq_(agent.Q, {})\n",
    "eq_(agent.V, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aff10ef64ad35c63d2e4b4bec75d2f99",
     "grade": true,
     "grade_id": "E-ActorCritic2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル2\n",
    "# check learning()\n",
    "agent.reset()\n",
    "agent.learning(s1=4, a1=\"right\", r=-1, s2=5)\n",
    "agent.learning(s1=5, a1=\"right\", r=10, s2=6)\n",
    "agent.learning(s1=3, a1=\"right\", r=-1, s2=4)\n",
    "agent.learning(s1=4, a1=\"right\", r=-1, s2=5)\n",
    "agent.learning(s1=5, a1=\"left\", r=-1, s2=4)\n",
    "agent.learning(s1=4, a1=\"right\", r=-1, s2=5)\n",
    "agent.learning(s1=5, a1=\"right\", r=10, s2=6)\n",
    "#print(agent.Q)\n",
    "#print(agent.V)\n",
    "\n",
    "ans_Q={(4, 'right'): 0.052000000000000005, (5, 'right'): 3.7199999999999998, (3, 'right'): -0.236, (5, 'left'): -0.6000000000000001}\n",
    "ans_V={4: 0.052000000000000005, 5: 3.12, 3: -0.236}\n",
    "\n",
    "assert all( (k,v) in agent.Q.items()\n",
    "            for (k,v) in ans_Q.items()\n",
    ")\n",
    "assert all( (k,v) in agent.V.items()\n",
    "            for (k,v) in ans_V.items()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aeef18bcff6cfb7002844e8791f9b237",
     "grade": true,
     "grade_id": "E-ActorCritic3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル\n",
    "# check get_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04990540a8407a83e6f9f5f7c769251e",
     "grade": true,
     "grade_id": "E-ActorCritic4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル\n",
    "# check reset()\n",
    "agent.reset()\n",
    "eq_(agent.Q, {})\n",
    "eq_(agent.V, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# MDPGridworld-v3\n",
    "\n",
    "</div>\n",
    "\n",
    "これまでのMDPGridworld-v1に，プチご褒美がおいてあるのがv3です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MDPGridworld-v3\")\n",
    "env.show_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の図のように，`C`と書かれているケーキがあります。\n",
    "ここに来ると美味しいケーキがあるので，5点もらえて，そこでエピソード終了になります。\n",
    "\n",
    "- **Dockerを利用している人は，`C`が出てこない人がいるかも知れません。その場合は西井に連絡してください。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここから下には，自由にセルを作成して，いろいろと動作確認して構いません。\n",
    "\n",
    "- ここまでに作成したエージェントを使って，`MDPGridworld-v3`の攻略をしてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
