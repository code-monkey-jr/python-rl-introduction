{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "# 名前欄\n",
    "\n",
    "</div>\n",
    "\n",
    "以下に名前と学籍番号(STUDENT_ID)の入力をして下さい。\n",
    "\n",
    "例)\n",
    "```python\n",
    "FAMILY_NAME = \"Yamaguchi\"   # 姓をローマ字で入力\n",
    "FIRST_NAME = \"Daisuki\"   #  名をローマ字で入力\n",
    "USER = \"a099bc\"          # 大学アカウントのユーザ名\n",
    "STUDENT_ID = \"20-2202-000-0\" # 省略なしで入力\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAMILY_NAME = \"\"   # 姓をローマ字で入力\n",
    "FIRST_NAME = \"\"   #  名をローマ字で入力\n",
    "USER = \"\"          # 大学アカウントのユーザ名\n",
    "STUDENT_ID = \"\"  # 12-2202-000-0 等と省略なしで入力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "# 注意\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "- 以下の説明文を読み， 内容を納得できるように自分で試しなさい。\n",
    "- **「# このセルで試してみよう」**, **「# このセルで動作確認をしよう」**と書かれているセルは自由に使って構いません。\n",
    "- 必要に応じてセルを増やしても構いません。役立ちそうな情報もメモしておくと，今後プログラムを組む上で便利です。\n",
    "- 表題等には適宜教科書のページを書いてあります。詳しい説明は教科書を参照しなさい。\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "## 設問と解答用セル\n",
    "    \n",
    "</div>   \n",
    "\n",
    "\"**Q.**\" から始まる設問に対する解答は必ず入力しなさい。解答用セルには以下のように表示される。\n",
    "\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "\n",
    "**注意**\n",
    "\n",
    "- <span class=\"mark\">raise NotImplementedError()の一行は**必ず消去**して</span>，<span class=\"mark\"># YOUR CODE HERE の下に解答</span>を書くこと。\n",
    "- <span class=\"mark\">\\# YOUR CODE HERE の一行は**消さない**</span>こと。どこに解答を書くべきかがわからなくなることがあります。\n",
    "- なんらかの演算をしたり，命令文を作ったときは，その**結果が予想通りか否かを必ず確認しなさい**\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "## 評価用セル\n",
    "    \n",
    "</div>\n",
    "\n",
    "- **「評価用セル」と書かれているセルは採点のためのセル**です。\n",
    "- **評価用コードが一部表示されている場合，それを実行すると課題の実行結果の確認**をできます。\n",
    "- **表示されていない評価用コードも**あります。\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "## 課題提出前の注意\n",
    "    \n",
    "</div>   \n",
    "\n",
    "**[重要]** 以下の手順で，全ての課題が正常に動作するか確認してから提出しましょう。\n",
    "\n",
    "1.  メニューバーの\"Kernel\"をクリック後，\"Restart\"を選択\n",
    "2. メニューバーの\"Cell\"を，\"Run All\"を選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5fc26672331a87fdee95a7911dc539d",
     "grade": false,
     "grade_id": "cell-8f738fa29708227f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# このセルは自動評価を行うための命令です。内容は無視して，実行(Shift-Enter)のみをしてください。\n",
    "#%%writefile unittests.py\n",
    "#%load unittests.py\n",
    "%run unittests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# MDPGridworld\n",
    "\n",
    "</div>\n",
    "\n",
    "- レポート課題の準備のための問題です。レポートはこのシートで準備したクラスや関数を用いて検討・考察した内容をまとめていただきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MDPGridworld-v1`:\n",
    "\n",
    "- [openAI Gym](https://gym.openai.com/)準拠のモジュール。迷路等をデザインして強化学習環境として利用できる。\n",
    "- ソースコードは[こちら](https://github.com/jnishii/gridworld-gym)。([IRLL/reinforcement_learning_class](https://github.com/IRLL/reinforcement_learning_class/tree/master/gym/envs)のものを改変したもの)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDPGridworldのモジュールの読み込みは次セルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MDPGridworld-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 状態と行動\n",
    "\n",
    "### 状態(state)\n",
    "エージェントは，自分がどの部屋にいるか(部屋ID)を状態(state)の情報として知ることができる。\n",
    "各部屋の配置および，状態(部屋ID)は以下の通り。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.show_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 行動(action)\n",
    "\n",
    "選択できる行動は以下に示す0から3の4通り。もし壁に向かって移動しようとしたら，その場に留まる。\n",
    "```python\n",
    "0 - Northに移動\n",
    "1 - Southに移動\n",
    "2 - Westに移動\n",
    "3 - Eastに移動\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### エピソード(episode)とエージェントの目的\n",
    "\n",
    "- **1エピソード**: スタート(S)から，ゴール(G)もしくはFの部屋に到達するまで\n",
    "- **エージェントの目的**: 1エピソード間にできるだけ多くの報酬を得ること"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openAI gymのインタンス変数・関数\n",
    "### 状態に関する情報を格納するインスタンス変数\n",
    "- `gym.observation_space` : 状態に関する情報\n",
    "- `gym.observation_space.n`: 状態数\n",
    "\n",
    "次セルでは各インスタンス変数の値を確認している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行結果より，状態が離散的な64個の値(0から63まで)をとることがわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行動に関するインスタンス変数\n",
    "- `gym.action_space`: 行動に関する情報\n",
    "- `gym.action_space.n`: 行動数\n",
    "- `gym.action_space.sample()`: エージェントがとりうる行動を一つランダムに選択して返す\n",
    "\n",
    "各インスタンス変数の値は以下の通り。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.n)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行結果より，とりうる行動は離散的な4個の値(0から3まで)で表現されていることがわかる。また，`env.action_space.sample()`の返り値は，この４個の値からランダムに選ばれている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Q.** 上述の`MDPGridWorld-v1`環境には，状態と行動の組み合わせは全部何通りあるか? その値を変数`ans`に代入しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e797f2f77a9f295ecc20acfe531e3686",
     "grade": false,
     "grade_id": "cell-cc03c48248013b48",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4db5a4f4526bffd77aa6078de2bcda0a",
     "grade": true,
     "grade_id": "E-gridworld-states",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他の情報\n",
    "\n",
    "`gym.spec.max_episode_steps`\n",
    "    - 1エピソード内の最大ステップ数\n",
    "    - ゴールに到達するか，もしくはこのステップ数に達したところでエピソードは終了\n",
    "**Q.** 上記のインスタンス変数の値を確認しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 環境クラスのインスタンス・メソッド\n",
    "\n",
    "- `env.reset()`: 初期化。エージェントの位置はSになる。\n",
    "- `env.step(action)`: \n",
    "    - 引数:\n",
    "        - `action` : エージェントのとる行動\n",
    "    - 返り値:\n",
    "        - `next_state` : actionによる状態遷移後の状態\n",
    "        - `reward` : actionにより得た報酬\n",
    "        - `terminal` : エピソード終了時にTrue, それ以外はFalse\n",
    "        - `info` : 行動actionによって，状態sから次の状態next_stateに遷移する確率(この演習では使わない)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "次セルで`env.reset()`を実行し，適当な`action`を用いて`env.step()`による状態遷移を行い，`action`に応じた適切な状態，報酬値，終了状態が得られることを確認しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下のactionを変更し，actionに応じたいろいろな返り値が得られていることを確認しなさい。\n",
    "env.reset()\n",
    "action = 0\n",
    "next_state, reward, terminal, info = env.step(action)\n",
    "# terminal is a return value of the environment's step method\n",
    "print(next_state, reward, terminal, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# MDPGridWorldを動き回る\n",
    "\n",
    "</div>\n",
    "\n",
    "## ランダムアクション\n",
    "前回の課題(rooms課題)で作ったクラス`Fool`および，`play_env()`の修正版を用いて`MDPGridWorld-v1`の環境を動き回るプログラムを作る。\n",
    "\n",
    "\n",
    "**Q(1)** 以前の課題(rooms課題)で作ったクラス`Fool`の定義を以下に入力しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルでは，実行に必要なモジュールの読み込みを行う。\n",
    "# ここに書かれているもの以外に必要なものがあれば，適宜追加すること。\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13f195292c82130f7c489fe9779f9eea",
     "grade": false,
     "grade_id": "cell-08d712ac2ed07e1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# class Foolの定義をこのセルに入力する\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23a92ce25f7118f840793bb42fcd5c0d",
     "grade": true,
     "grade_id": "E-gridworld-Fool",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q(2)** 以前の課題(rooms課題シート)で作った`play_env()`を以下のよう修正しなさい。\n",
    "\n",
    "**変更点**\n",
    "1. 引数`max_step`(1episode内の最大ステップ数)は廃止。`play_env()`内での処理には，代わりに`env.spec.max_episode_steps` の値を用いること。\n",
    "2. 引数`s0`も廃止\n",
    "3. 関数`env.step(action)`の返り値はこれまで`state,reward,terminal`の3つだったが，openAI gymでは4つめに`info`が加わる。ただ，この引数の内容を使うことはほとんど無いので，以下のように返り値を受け取って，4つ目の引数は無視する。\n",
    "```python\n",
    "s,r,terminal,_=env.step(action) # 返り値の第4要素は'_'という名前の変数で受け取っている。\n",
    "```\n",
    "`play_env()`を呼び出して，ランダム選択エージェント`Fool`が`FronzenLake-v1`の環境を動き回る実行例は，解答欄の次のセルにある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3728a9906f097f108e2aa7a85a384fdb",
     "grade": false,
     "grade_id": "cell-62f3023136ba8250",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def play_env(env, agent, learning=True, max_episode=30, render_step=20, verbose=False):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `play_env()`を呼び出して，ランダム選択エージェントFoolが`MDPGridworld-v1`の環境を動き回る実行例。\n",
    "# 引数をいろいろと変えて，動作検証をしなさい。\n",
    "\n",
    "env = gym.make(\"MDPGridworld-v1\")\n",
    "action_list = list(range(env.action_space.n))\n",
    "step, score = play_env(\n",
    "    env,\n",
    "    agent=Fool(action_list),\n",
    "    learning=False,\n",
    "    max_episode=1,\n",
    "    render_step=1,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6a075e205bd18d1bf3a068c51b78702",
     "grade": true,
     "grade_id": "E-gridworld-play_env",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル(Foolを用いた動作確認。demo_env()と同様の動作確認)\n",
    "env = gym.make(\"MDPGridworld-v1\")\n",
    "action_list = list(range(env.action_space.n))\n",
    "\n",
    "\n",
    "def test_mdpgrid(agent, verbose=True):\n",
    "    return play_env(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        learning=False,\n",
    "        max_episode=1,\n",
    "        render_step=1,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "result = [test_mdpgrid(Fool(action_list, 0), verbose=False) for i in range(2)]\n",
    "eq_(result, ([([200], [-600]), ([200], [-600])]))\n",
    "result = [test_mdpgrid(Fool(action_list, 1), verbose=False) for i in range(2)]\n",
    "eq_(result, ([([200], [-600]), ([200], [-600])]))\n",
    "result = test_mdpgrid(Fool(action_list, 2), verbose=False)\n",
    "eq_(result, ([200], [-600]))\n",
    "result = test_mdpgrid(Fool(action_list, 3), verbose=False)\n",
    "eq_(result, ([200], [-600]))\n",
    "result = [test_mdpgrid(Fool(action_list, \"random\"), verbose=False) for i in range(10)]\n",
    "result2 = [test_mdpgrid(Fool(action_list, \"random\"), verbose=False) for i in range(10)]\n",
    "assert_not_equal(result, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以前の課題(rooms課題)で作った関数`plot_history()`を使えば，エピソードごとのステップ数や総得点の履歴のグラフ表示もできる。\n",
    "\n",
    "**Q.** 次のセルにclass課題で作った`plot_history()`を入力して動作確認を行い，必要に応じて修正をしなさい。動作確認のための実行例は解答欄の次のセルにある。\n",
    "\n",
    "注意)\n",
    "- class課題で作った関数`plot_history()`で定義した`ax1`,`ax2`の変数名は変更しないこと。\n",
    "- 返り値も`ax1, ax2`のまま変更しないこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "045ef0b0dd2d004b51dfa76c5861b59b",
     "grade": false,
     "grade_id": "cell-451de4f61c742e83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行例\n",
    "# 引数をいろいろと変えて，動作検証をしなさい。\n",
    "env = gym.make(\"MDPGridworld-v1\")\n",
    "step, score = play_env(\n",
    "    env,\n",
    "    agent=Fool(action_list),\n",
    "    learning=False,  # Foolは学習しない\n",
    "    max_episode=100,\n",
    "    verbose=False,\n",
    ")\n",
    "plot_history(step, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "653da07559bb2767d151934287225616",
     "grade": true,
     "grade_id": "E-gridworld-plot_history",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフのファイル保存\n",
    "\n",
    "前問のブログラムの場合，`plt.tight_layout()`の直後に以下の命令を追加すると**グラフをファイル保存**できる。\n",
    "```python\n",
    "plt.savefig('figure.png')\n",
    "```\n",
    "保存されたファイルは，課題シートの一覧画面で確認やダウンロードをできる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q学習\n",
    "**Q.** 以前の課題(rooms)で作ったクラス`Qlearner`と，このシートで定義済みの関数`play_env()`を使えば，MDPGridWorld環境に対する学習もできるはずである。次のセルに`Qlearner`の定義を入力して動作確認を行い，必要に応じて修正しなさい。\n",
    "\n",
    "実行例は解答欄の次のセルにある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94fda1d3a72560827e500b6ecbdde315",
     "grade": false,
     "grade_id": "cell-3e4bacb41b3355e1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 以下にclass QLearnerの定義を書きなさい。(必要ならindex_list()等, Qlearnerの実行に必要な関数も書きなさい)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e444a8a4b8dc2d65f1c1ecbe82b927e",
     "grade": true,
     "grade_id": "E-gridworld-Qlearner",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の継続\n",
    "Q学習エージェント`agent`の学習結果(Q値)は，関数`play_env()`を終了しても変数`agent`に保持される。したがって，上のセルの実行後に，以下のように`play_env()`を再度実行すれば，上記の学習の続きを行うことができる。\n",
    "```python\n",
    "step,score=play_env(env, agent=agent, max_episode=100, verbose=False)\n",
    "plot_history(step,score)\n",
    "```\n",
    "**学習結果(Q値)を未学習な状態に戻したいとき**は，`agent`を再定義するか，`agent.reset()`を実行する。\n",
    "\n",
    "**Q.** 実際に`play_env()`を何度か実行してみて，未学習の状態よりも，パフォーマンスが良くなっていくことを確認しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, score = play_env(env, agent=agent, max_episode=100, verbose=False)\n",
    "plot_history(step, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Q学習エージェントは学習により，どのようなQ値を獲得しただろうか。可視化を試みよ。例えば，Q値を格納している辞書`agent.Q`の内容を見ると，獲得したQ値の一覧を確認できる。以下は，可視化のサンプルプログラムだが，このサンプルプログラムをレポート等に利用したい場合には，座標軸の値やタイトルを加え，また，表示内容に関する十分な解説を加えて使うこと。プログラムの内容を理解しないまま，図だけ見て意味を判断して使うと，勘違いをすることがあるので要注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_Q(agent, n_col=8):\n",
    "    Qarray = []\n",
    "    Qrow = []\n",
    "    nCol = n_col\n",
    "    print(\"+--------+\")\n",
    "    print(\"|\", end=\"\")\n",
    "    for s in range(env.observation_space.n):\n",
    "        q = [agent.get_q(s, a) for a in range(env.action_space.n)]\n",
    "        Qrow.append(max(q))\n",
    "        argmaxQ = q.index(max(q))  # Q値が最大になるaのうち, indexが最小のもの\n",
    "        print(\"{}\".format([\"^\", \"v\", \"<\", \">\"][argmaxQ]), end=\"\")\n",
    "        if (s + 1) % nCol == 0:\n",
    "            print(\"|\")\n",
    "            if s != env.observation_space.n - 1:\n",
    "                print(\"|\", end=\"\")\n",
    "            Qarray.append(Qrow)\n",
    "            Qrow = []\n",
    "    print(\"+--------+\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 5))  # グラフを生成\n",
    "    ax = fig.subplots()  # 座標軸を1つ生成してaxに代入\n",
    "    ax.invert_yaxis()    # y軸を上下ひっくり返したいとき\n",
    "\n",
    "    tick = range(0, 8, 1)\n",
    "    ax.set_xticks(tick)\n",
    "    ax.set_yticks(tick)\n",
    "\n",
    "    map = ax.pcolor(Qarray, cmap=\"bwr\")  # 数値データからカラーデータを作成 cmapの値により配色変化\n",
    "    fig.colorbar(map, ax=ax)  # カラーデータと軸データを使ってグラフデータ生成\n",
    "\n",
    "    plt.show()  # 表示(生成されているグラフデータを画面表示)\n",
    "\n",
    "\n",
    "show_Q(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. `class Qlearner`の継承クラスとして，`class SARSAlearner`を作り，学習のためのインスタンス関数`learning()`の内容をSARSA学習にしなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b4bec4bab13156003316794c1d41da5",
     "grade": false,
     "grade_id": "cell-d04558d0637745a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# このセルにクラス定義を入力する\n",
    "class SARSAlearner(Qlearner):\n",
    "    def learning(self, s1, a1, r, s2, terminal=False):\n",
    "        \"\"\"\n",
    "        Q(s1, a1) <= Q(s1, a1) + alpha * (r + gamma*Q(s2,a') - Q(s1,a1)) # 通常時\n",
    "        Q(s1, a1) <= Q(s1, a1) + alpha * (r - Q(s1,a1)) # エピソード終了時\n",
    "        a'は，状態s'において行動選択ポリシーに従い選択される行動(get_action()で取得)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルで動作確認をしよう\n",
    "# SARSAlearner + play_env() + MGPGridWorld の学習プログラム\n",
    "\n",
    "agent = SARSAlearner(action_list)  # 学習エージェント生成\n",
    "step, score = play_env(env=env, agent=agent, max_episode=100, verbose=False)\n",
    "plot_history(step, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c544ffe25decc4a34b5d54c3d718a236",
     "grade": true,
     "grade_id": "E-gridworld-SARSAlearner",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 評価用セル\n",
    "class SARSAlearnerTest(SARSAlearner):\n",
    "    def set_action(self, a=0):\n",
    "        self.def_action = a\n",
    "\n",
    "    def get_action(self, s):\n",
    "        return self.def_action\n",
    "\n",
    "\n",
    "env = gym.make(\"MDPGridworld-v1\")\n",
    "action_list = list(range(env.action_space.n))  # action_list取得\n",
    "agent = SARSAlearnerTest(action_list)  # 学習エージェント生成\n",
    "\n",
    "# test1\n",
    "agent.set_action(a=0)\n",
    "step, score = play_env(env=env, agent=agent, max_episode=5, verbose=False)\n",
    "ans_Q = {\n",
    "    (56, 0): -2.882376096,\n",
    "    (48, 0): -2.882376096,\n",
    "    (40, 0): -2.882376096,\n",
    "    (32, 0): -2.9070494918446594,\n",
    "    (24, 0): -3.3473247916120967,\n",
    "    (16, 0): -6.290059022035771,\n",
    "    (8, 0): -15.00201805496299,\n",
    "    (0, 0): -24.509190126958917,\n",
    "}\n",
    "eq_(agent.Q, ans_Q)\n",
    "# test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "# ここから下には，プログラムを書かないでください\n",
    "\n",
    "</div>    \n",
    "    \n",
    "プログラムのテストは，これまでに提出済みのシートに追加セルを作成して行ってください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
