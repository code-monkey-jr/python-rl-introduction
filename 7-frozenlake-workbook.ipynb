{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 宿題シート: FrozenLake\n",
    "\n",
    "- このシートにはレポート課題に関する説明と，レポート課題の準備のための問題，そしてレポート課題の説明があります。\n",
    "- このシート自体を提出する必要はありません。レポート課題については別のシートを見てください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定\n",
    "以下は，利用する環境の各種設定。内容は気にせずに実行してください。\n",
    "```\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78,\n",
    ")\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200,\n",
    "    reward_threshold=0.99, # optimum = 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake\n",
    "\n",
    "- [openAI Gym](https://gym.openai.com/)のモジュール\n",
    "\n",
    "## FrozenLakeモジュールの読み込みとクラス定義\n",
    "\n",
    "```\n",
    "import gym\n",
    "env = gym.make('FrozenLake-v0') \n",
    "```\n",
    "\n",
    "## **Actions and states**\n",
    "\n",
    "- see [FrozenLake-v0](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py)\n",
    "\n",
    "\n",
    "ソースコードにある説明文\n",
    "```\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "```\n",
    "\n",
    "各状態は以下のように0から15の数値で表されている。\n",
    "```\n",
    " 0  1  2  3\n",
    " 4  5  6  7\n",
    " 8  9 10 11\n",
    "12 13 14 15\n",
    "```\n",
    "行動は以下の通り\n",
    "```\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openAI gymのインタンス変数・関数\n",
    "### 状態に関する情報を格納するインスタンス変数\n",
    "- `gym.observation_space` : 状態に関する情報\n",
    "- `gym.observation_space.n`: 状態数\n",
    "\n",
    "表示例\n",
    "```python\n",
    "print(env.observation_space) \n",
    "print(env.observation_space.n)\n",
    "```\n",
    "\n",
    "**Q.** 上記表示例を実行して，その内容を確認しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行動に関するインスタンス変数\n",
    "- `gym.action_space`: 行動に関する情報\n",
    "- `gym.action_space.n`: 行動数\n",
    "\n",
    "**Q.** 上記の各インスタンス変数の値を表示し，その意味を確認しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** 状態と行動の組み合わせは全部何通りあるか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他の情報\n",
    "\n",
    "- `gym.spec.max_episode_steps`: 1エピソード内の最大ステップ数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境のインスタンス・メソッド\n",
    "\n",
    "- `env.reset()`: 初期化。エージェントの位置はSになる。\n",
    "- `env.step(action)`: \n",
    "    - 引数:\n",
    "        - `action` : エージェントのとる行動\n",
    "    - 返り値:\n",
    "        - `next_state` : actionによる状態遷移後の状態\n",
    "        - `reward` : actionにより得た報酬\n",
    "        - `terminal` : エピソード終了時にTrue, それ以外はFalse\n",
    "        - `info` : 行動actionによって，状態sから次の状態next_stateに遷移する確率(この演習では使わない)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** `env.reset()`を実行しなさい。その後，適当な`action`を用いて`env.step()`による状態遷移を行い，`action`に応じた適切な状態，報酬値，終了状態が得られることを確認しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrozenLakeNotSlippery\n",
    "\n",
    "### ランダムアクション\n",
    "\n",
    "**Q.** 以下は，ランダム選択エージェントが，`FronzenLake-v0`の環境を動き回るプログラム例である。\n",
    "このプログラムを完成し，エージェントの行動遷移が適切に表示されることを確認しなさい。\n",
    "\n",
    "注) openAI Gym標準の`FrozenLake-v0`では足元が滑りやすい(選択した行動は確率的にしか実行されない)ので，まずは，選択した行動が確実が実行されるように修正した`FrozenLakeNonslippy-v0`を使う。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random as ra\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "\n",
    "class Fool:\n",
    "    # 前回の宿題(6-class)で定義したものを使う\n",
    "\n",
    "def play_env(env, agent, max_episode=100, learning=True, render_step=20, verbose=False):\n",
    "\"\"\"    \n",
    "    前回の宿題(6-class)で作ったものを，必要に応じて修正して使いなさい。(修正すべき点は少ないはず)\n",
    "    ただし，1episode内の最大ステップ数(max_step)は引数とせず，env.spec.max_episode_steps を用いる。\n",
    "\"\"\"\n",
    "\n",
    "env16_nonSlip=gym.make('FrozenLakeNotSlippery-v0')\n",
    "action_list=list(range(env16_nonSlip.action_space.n))\n",
    "step,score=play_env(env=env16_nonSlip, \n",
    "                    agent=Fool(action_list), \n",
    "                    learning=False, \n",
    "                    max_episode=1, \n",
    "                    render_step=1, \n",
    "                    verbose=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** 以前の課題(6-class)で作った関数`plot_history()`を，必要に応じて修正して使えば，エピソードごとのステップ数や総得点の履歴のグラフ表示もできるはずである。実際にプログラムを作り，グラフ表示をしてみなさい。\n",
    "\n",
    "```python\n",
    "def plot_history # ここから自分で作る\n",
    "\n",
    "# 実行\n",
    "step,score=play_env(env=env16_nonSlip, \n",
    "                    agent=Fool(action_list), \n",
    "                    learning=False, \n",
    "                    max_episode=100, \n",
    "                    verbose=False)\n",
    "plot_history(step,score)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q学習\n",
    "**Q.** 以前の課題(6-class)で作ったクラス`Qlearner`, 学習プログラム`learn_env()`を使えば，FrozenLake環境に対する学習もできるはずである。必要に応じて修正をして，実際に学習を行い，学習曲線のグラフを描きなさい。\n",
    "```python\n",
    "import random as ra\n",
    "index_list = lambda L, value: [i for i,x in enumerate(L) if x==value]\n",
    "\n",
    "class Qlearner:\n",
    "    # 自分で作る\n",
    "    \n",
    "def learn_env(env, agent, max_episode=100, render_step=20, verbose=False):\n",
    "    # 自分で作る\n",
    "\n",
    "```\n",
    "\n",
    "以上の定義をした後，以下を実行すれば学習曲線のグラフが表示されるはず。\n",
    "```\n",
    "# 環境，エージェント等の定義\n",
    "env16_nonSlip=gym.make('FrozenLakeNotSlippery-v0')\n",
    "action_list=list(range(env16_nonSlip.action_space.n))\n",
    "q_agent=Qlearner(action_list)\n",
    "\n",
    "# 実行\n",
    "step,score=play_env(env=env16_nonSlip, agent=q_agent, max_episode=100, verbose=False)\n",
    "plot_history(step,score)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の継続\n",
    "`q_agent`の学習結果(Q値)は，`play_env()`を終了しても保持される。したがって，以下のように`play_env()`を再度実行すれば，上記の学習の続きを行うことができる。\n",
    "```\n",
    "step,score=play_env(env=env16, agent=q_agent, max_episode=100, verbose=False)\n",
    "plot_history(step,score)\n",
    "```\n",
    "**学習結果(Q値)を未学習な状態に戻したいとき**は，`q_agent`を再定義するか，`q_agent.reset()`を実行する。\n",
    "\n",
    "**Q.** 実際に何度か実行してみて，未学習の状態よりも，パフォーマンスが良くなっていくことを確認しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** 学習エージェント`q_agent`は学習により，どのようなQ値を獲得しただろうか。可視化を試みよ。例えば，`q_agent.Q`の内容を見ると，獲得したQ値の一覧を確認できる。また，以下は，可視化の例のヒントになるかもしれないサンプルプログラムだが，このサンプルプログラムをレポート等に利用したい場合には，座標軸の値やタイトル，表示に内容に関する十分な解説を加えて使うこと。内容を理解しないまま，図だけ見て意味を判断して使うと，大きな勘違いをすることがあるので要注意。\n",
    "\n",
    "```python\n",
    "Qarray=[]\n",
    "Qrow=[]\n",
    "\n",
    "print(\"+----+\")\n",
    "print(\"|\",end='')\n",
    "for s in range(env.observation_space.n):\n",
    "    q = [q_agent.get_q(s, a) for a in range(env.action_space.n)]\n",
    "    Qrow.append(max(q))\n",
    "    argmaxQ=q.index(max(q))\n",
    "    print(\"{}\".format([\"<\",\"v\",\">\",\"^\"][argmaxQ]), end='')\n",
    "    if (s+1) % 4 == 0:\n",
    "        print(\"|\")\n",
    "        if s!= env.observation_space.n - 1:\n",
    "            print(\"|\",end='')\n",
    "        Qarray.append(Qrow)\n",
    "        Qrow=[]\n",
    "print(\"+----+\")\n",
    "\n",
    "plt.imshow(Qarray,aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrozenLake\n",
    "\n",
    "**Q**. 今度は，滑りやすいFrozenLake環境(`FrozenLake-v0`)で，選択した行動が実現される確率はどの程度を確認しなさい。正解は，このシート冒頭のリンクから見れる`FrozenLake`のソースコードを見ればわかる。\n",
    "```\n",
    "env16 = gym.make('FrozenLake-v0')\n",
    "action_list=list(range(env16.action_space.n))\n",
    "q_agent=Qlearner(action_list)\n",
    "\n",
    "step,score=play_env(env=env16, agent=q_agent, learning=False, max_episode=1, render_step=1, verbose=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**. 滑りやすいFrozenLake環境での学習パフォーマンスをグラフにして確認しなさい。\n",
    "```\n",
    "step,score=play_env(env=env16_slippery, agent=q_agent, max_episode=200, verbose=False)\n",
    "plot_history(step,score)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake8x8\n",
    "\n",
    "8x8のFrozenLake環境もある。\n",
    "```python\n",
    "MAPS = {\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "}\n",
    "```\n",
    "**Q.** 上記の例では，Hが何個あるか確認しなさい。また，状態と行動の組み合わせはいくつあるか答えなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrozenLakeNotSlippery-8x8\n",
    "決定論的な状態遷移をする環境は以下で定義できる。\n",
    "```\n",
    "env64_nonSlip = gym.make('FrozenLakeNotSlippery8x8-v0')\n",
    "```\n",
    "**Q.** Qエージェントによる学習シミュレーションを行い，結果をグラフ表示しなさい。果たして，学習は成功しているだろうか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の改善\n",
    "\n",
    "1. **Q値の初期値**(`q0`)を変えると，学習にはどのような影響があるだろうか? その理由はなんだろうか?\n",
    "2. **報酬設定**を工夫すると，学習速度を速くできるだろうか。報酬設定を変えるときには，エージェントのクラス`Qlearner`を修正し，インスタンス・メソッド`learning()`において，引数で与えられる状態遷移や報酬情報をもとに再設定すると良い。つまり，環境から得られる報酬は変わらないが，エージェント自身が感じる評価値は別に用意する。(千円もらっても，人によって嬉しさが違うのと同じ様な状況設定)\n",
    "3. 学習の速度を速くする工夫をするには，どんな方法がありえるだろうか? いくつか考えて試しなさい。\n",
    "\n",
    "なお，クラス`Qlearner`のインスタンス・メソッド`learning()`のみを変えたクラス`Qlearner2`を作るときには，以下のようにクラス継承を利用すると良い。修正点のみの記述ですむ。(テキストp.288)\n",
    "\n",
    "```\n",
    "class Qlearner2(Qlearner): # Qlearner を継承した Qlearner2 を宣言\n",
    "    def learning(self, s1, a1, r, s2, terminal=False):  # 修正したい関数のみを定義し直す\n",
    "        # 以下には命令文\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## FrozenLakeNotSlippery-8x8\n",
    "\n",
    "次は，すべりやすい(非決定論的)状態遷移をする環境`FrozenLake8x8-v0`で試そう。環境定義は以下のように行う。\n",
    "```\n",
    "env64 = gym.make('FrozenLake8x8-v0')\n",
    "```\n",
    "**Q.** Qエージェントによる学習シミュレーションを行い，結果をグラフ表示しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習効果の検討\n",
    "\n",
    "**Q8.** 以下を検討しなさい。いずれも，レポートを書く際に重要な検討事項になる点である。\n",
    "\n",
    "1. 作成した学習プログラムを使って，100エピソードの間の学習によるパフォーマンスの変化を何度か観察してみなさい。100エピソード間の評価の変化は毎回同じだろうか?\n",
    "2. 作成した学習プログラムで得た結果は，ランダムに行動選択するエージェントに比べて良い成績を得ることをできるようになったことを確実に示すものだろうか。疑い深いヒトに対して，学習が成功していることを納得させられるように説明するには，どのような根拠(証拠)に基づいて説明したら良いだろうか。いくつかの方法を考えなさい。\n",
    "2. **Q値の初期値**(`q0`)を変えると，学習にはどのような影響があるだろうか? その理由はなんだろうか?\n",
    "3. **報酬量**を変化させると，学習の速度はどのように変化するだろうか?\n",
    "4. 行動選択の**ポリシー**は改善の余地はあるだろうか?\n",
    "5. **学習方法**には改善の余地はあるだろうか?\n",
    "6. 学習の速度を速くする工夫をするには，上記の他に，どんな方法がありえるだろうか? いくつか考えて試しなさい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
